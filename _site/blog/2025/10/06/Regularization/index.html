<html>
<head>
    <title>Inductive Bias and the Weight Competition</title>
    <meta charset='UTF-8'>
    <meta content='width=device-width, initial-scale=1' name='viewport'/>

    <meta name='description' content='Lasse Arpe Kristensen.'>
    <!-- <meta name='keywords' content='machine-learning'> -->
    <meta name='author' content='Lasse Arpe Kristensen''>

    <link href='/assets/blog.css' rel='stylesheet'/>

    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">


    <!-- <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"></script>
    <script>
      document.addEventListener("DOMContentLoaded", function() {
          renderMathInElement(document.body, {
          // customised options
          // • auto-render specific keys, e.g.:
          delimiters: [
              {left: '$$', right: '$$', display: true},
              {left: '$', right: '$', display: false},
              {left: '\\(', right: '\\)', display: false},
              {left: '\\[', right: '\\]', display: true}
          ],
          // • rendering keys, e.g.:
          throwOnError : false  
        });
    });
  </script> -->

</head>
<body>
<div class='content'>
    <div class='nav'>
    <ul class='wrap'>
        <li><a href='/'>Home</a></li>
        <li><a href='/blog'>Blog</a></li>
        <li><a href='/feed.xml'>RSS</a></li>
    </ul>
</div>
    <div class='front-matter'>
        <div class='wrap'>
            <h1>Inductive Bias and the Weight Competition</h1>
            <h4>In essence, regularization can be viewed as a competition between weights.</h4>
            <div class='bylines'>
                <div class='byline'>
                    <h3>Published</h3>
                    <p>06 October 2025</p>
                </div>
            </div>
            <div class='clear'></div>
        </div>
    </div>
    <div class='wrap article'>
        <h1 id="inductive-bias-and-weight-competition">Inductive Bias and Weight Competition</h1>

<p>In machine learning, we are interested in a low generalization error. The training error can be arbitrarily optimized emperically, hence the problem of empirical risk minimization.</p>

<p>The interesting thing is how we will inference on unseen data. This is noted as the generalization error.</p>

<p>To achieve a low generalization error, we are deliberately introducing bias.</p>

<p>An example of this is the weight penalization through a regularization term.</p>

<p>When we introduce a regularization term, we are asking our model to <em>choose</em> between weights.  Remember, we want our loss to be as close to zero as possible. If we <em>add</em> the very same weight back into the model, it forces the algorithm to choose weigts differently. To choose weights within a given budget.</p>

<p>Each loss point is the result of the sum of our weighted features. This can be for instance be the absolute value (in ridge, L1), or the squared (in LASSO, L2).</p>


    </div>

</div>
</body>
</html>
